{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Import the Dependencies At The Top of The Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import quandl\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also import Plotly and enable the offline mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "py.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Retrieve Bitcoin Pricing Data\n",
    "\n",
    "Now that everything is set up, we're ready to start retrieving data for analysis. First, we need to get Bitcoin pricing data using Quandl's free Bitcoin API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2.1 - Define Quandl Helper Function\n",
    "\n",
    "To assist with this data retrieval we'll define a function to download and cache datasets from Quandl.\n",
    "We're using pickle to serialize and save the downloaded data as a file, which will prevent our script from re-downloading the same data each time we run the script. The function will return the data as a Pandas dataframe. \n",
    "If you're not familiar with dataframes, you can think of them as super-powered spreadsheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quandl_data(quandl_id):\n",
    "    '''Download and cache Quandl dataseries'''\n",
    "    cache_path = '{}.pkl'.format(quandl_id).replace('/','-')\n",
    "    try:\n",
    "        f = open(cache_path, 'rb')\n",
    "        df = pickle.load(f)   \n",
    "        print('Loaded {} from cache'.format(quandl_id))\n",
    "    except (OSError, IOError) as e:\n",
    "        print('Downloading {} from Quandl'.format(quandl_id))\n",
    "        df = quandl.get(quandl_id, returns=\"pandas\")\n",
    "        df.to_pickle(cache_path)\n",
    "        print('Cached {} at {}'.format(quandl_id, cache_path))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2 - Pull Kraken Exchange Pricing Data\n",
    "\n",
    "Let's first pull the historical Bitcoin exchange rate for the Kraken Bitcoin exchange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull Kraken BTC price exchange data\n",
    "btc_usd_price_kraken = get_quandl_data('BCHARTS/WEXUSD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_usd_price_kraken.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll generate a simple chart as a quick visual verification that the data looks correct.\n",
    "Here, we're using Plotly for generating our visualizations. This is a less traditional choice than some of the more established Python data visualization libraries such as Matplotlib, but I think Plotly is a great choice since it produces fully-interactive charts using D3.js. These charts have attractive visual defaults, are easy to explore, and are very simple to embed in web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart the BTC pricing data\n",
    "btc_trace = go.Scatter(x=btc_usd_price_kraken.index, y=btc_usd_price_kraken['Weighted Price'])\n",
    "py.iplot([btc_trace])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.3 - Pull Pricing Data From More BTC Exchanges\n",
    "\n",
    "You might have noticed a hitch in this dataset - there are a few notable down-spikes, particularly in late 2014 and early 2016. These spikes are specific to the Kraken dataset, and we obviously don't want them to be reflected in our overall pricing analysis.\n",
    "\n",
    "The nature of Bitcoin exchanges is that the pricing is determined by supply and demand, hence no single exchange contains a true \"master price\" of Bitcoin. To solve this issue, along with that of down-spikes (which are likely the result of technical outages and data set glitches) we will pull data from three more major Bitcoin exchanges to calculate an aggregate Bitcoin price index.\n",
    "\n",
    "First, we will download the data from each exchange into a dictionary of dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull pricing data for 3 more BTC exchanges\n",
    "exchanges = ['COINBASE','BITSTAMP','ITBIT']\n",
    "\n",
    "exchange_data = {}\n",
    "\n",
    "exchange_data['KRAKEN'] = btc_usd_price_kraken\n",
    "\n",
    "for exchange in exchanges:\n",
    "    exchange_code = 'BCHARTS/{}USD'.format(exchange)\n",
    "    btc_exchange_df = get_quandl_data(exchange_code)\n",
    "    exchange_data[exchange] = btc_exchange_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.4 - Merge All Of The Pricing Data Into A Single Dataframe\n",
    "\n",
    "Next, we will define a simple function to merge a common column of each dataframe into a new combined dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dfs_on_column(dataframes, labels, col):\n",
    "    '''Merge a single column of each dataframe into a new combined dataframe'''\n",
    "    series_dict = {}\n",
    "    for index in range(len(dataframes)):\n",
    "        series_dict[labels[index]] = dataframes[index][col]\n",
    "        \n",
    "    return pd.DataFrame(series_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will merge all of the dataframes together on their \"Weighted Price\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the BTC price dataseries' into a single dataframe\n",
    "btc_usd_datasets = merge_dfs_on_column(list(exchange_data.values()), list(exchange_data.keys()), 'Weighted Price')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can preview last five rows the result using the tail() method, to make sure it looks ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_usd_datasets.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.5 - Visualize The Pricing Datasets\n",
    "\n",
    "The next logical step is to visualize how these pricing datasets compare. For this, we'll define a helper function to provide a single-line command to generate a graph from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_scatter(df, title, seperate_y_axis=False, y_axis_label='', scale='linear', initial_hide=False):\n",
    "    '''Generate a scatter plot of the entire dataframe'''\n",
    "    label_arr = list(df)\n",
    "    series_arr = list(map(lambda col: df[col], label_arr))\n",
    "    \n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        legend=dict(orientation=\"h\"),\n",
    "        xaxis=dict(type='date'),\n",
    "        yaxis=dict(\n",
    "            title=y_axis_label,\n",
    "            showticklabels= not seperate_y_axis,\n",
    "            type=scale\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    y_axis_config = dict(\n",
    "        overlaying='y',\n",
    "        showticklabels=False,\n",
    "        type=scale )\n",
    "    \n",
    "    visibility = 'visible'\n",
    "    if initial_hide:\n",
    "        visibility = 'legendonly'\n",
    "        \n",
    "    # Form Trace For Each Series\n",
    "    trace_arr = []\n",
    "    for index, series in enumerate(series_arr):\n",
    "        trace = go.Scatter(\n",
    "            x=series.index, \n",
    "            y=series, \n",
    "            name=label_arr[index],\n",
    "            visible=visibility\n",
    "        )\n",
    "        \n",
    "        # Add seperate axis for the series\n",
    "        if seperate_y_axis:\n",
    "            trace['yaxis'] = 'y{}'.format(index + 1)\n",
    "            layout['yaxis{}'.format(index + 1)] = y_axis_config    \n",
    "        trace_arr.append(trace)\n",
    "\n",
    "    fig = go.Figure(data=trace_arr, layout=layout)\n",
    "    py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now easily generate a graph for the Bitcoin pricing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all of the BTC exchange prices\n",
    "df_scatter(btc_usd_datasets, 'Bitcoin Price (USD) By Exchange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.6 - Clean and Aggregate the Pricing Data\n",
    "\n",
    "We can see that, although the four series follow roughly the same path, there are various irregularities in each that we'll want to get rid of.\n",
    "\n",
    "Let's remove all of the zero values from the dataframe, since we know that the price of Bitcoin has never been equal to zero in the timeframe that we are examining.\n",
    "\n",
    "When we re-chart the dataframe, we'll see a much cleaner looking chart without the down-spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"0\" values\n",
    "btc_usd_datasets.replace(0, np.nan, inplace=True)\n",
    "# Plot the revised dataframe\n",
    "df_scatter(btc_usd_datasets, 'Bitcoin Price (USD) By Exchange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate a new column, containing the average daily Bitcoin price across all of the exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average BTC price as a new column\n",
    "btc_usd_datasets['avg_btc_price_usd'] = btc_usd_datasets.mean(axis=1)\n",
    "# Plot the average BTC price\n",
    "btc_trace = go.Scatter(x=btc_usd_datasets.index, y=btc_usd_datasets['avg_btc_price_usd'])\n",
    "py.iplot([btc_trace])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Retrieve Altcoin Pricing Data\n",
    "\n",
    "Now that we have a solid time series dataset for the price of Bitcoin, let's pull in some data for non-Bitcoin cryptocurrencies, commonly referred to as altcoins.\n",
    "\n",
    "## Step 3.1 - Define Poloniex API Helper Functions\n",
    "\n",
    "For retrieving data on cryptocurrencies we'll be using the Poloniex API. To assist in the altcoin data retrieval, we'll define two helper functions to download and cache JSON data from this API.\n",
    "\n",
    "First, we'll define get_json_data, which will download and cache JSON data from a provided URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_data(json_url, cache_path):\n",
    "    '''Download and cache JSON data, return as a dataframe.'''\n",
    "    try:        \n",
    "        f = open(cache_path, 'rb')\n",
    "        df = pickle.load(f)   \n",
    "        print('Loaded {} from cache'.format(json_url))\n",
    "        print(cache_path)\n",
    "    except (OSError, IOError) as e:\n",
    "        print('Downloading {}'.format(json_url))\n",
    "        df = pd.read_json(json_url)\n",
    "        df.to_pickle(cache_path)\n",
    "        print('Cached {} at {}'.format(json_url, cache_path))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a function that will generate Poloniex API HTTP requests, and will subsequently call our new get_json_data function to save the resulting data.\n",
    "This function will take a cryptocurrency pair string (such as 'BTC_ETH') and return a dataframe containing the historical exchange rate of the two currencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_polo_url = 'https://poloniex.com/public?command=returnChartData&currencyPair={}&start={}&end={}&period={}'\n",
    "start_date = datetime.strptime('2017-10-01', '%Y-%m-%d') # get data from the start of november\n",
    "end_date = datetime.now() # up until today\n",
    "period = 300 # pull data for every 5'\n",
    "\n",
    "def get_crypto_data(poloniex_pair):\n",
    "    '''Retrieve cryptocurrency data from poloniex'''\n",
    "    json_url = base_polo_url.format(poloniex_pair, start_date.timestamp(), end_date.timestamp(), period)\n",
    "    data_df = get_json_data(json_url, poloniex_pair)\n",
    "    data_df = data_df.set_index('date')\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2 - Download Trading Data From Poloniex\n",
    "\n",
    "Most altcoins cannot be bought directly with USD; to acquire these coins individuals often buy Bitcoins and then trade the Bitcoins for altcoins on cryptocurrency exchanges. For this reason, we'll be downloading the exchange rate to BTC for each coin, and then we'll use our existing BTC pricing data to convert this value to USD.\n",
    "\n",
    "We'll download exchange data for nine of the top cryptocurrencies -\n",
    "Ethereum, Litecoin, Ripple, Ethereum Classic, Stellar, Dash, Siacoin, Monero, and NEM.\n",
    "\n",
    "Now we have a dictionary with 9 dataframes, each containing the historical daily average exchange prices between the altcoin and Bitcoin.\n",
    "\n",
    "We can preview the last few rows of the Ethereum price table to make sure it looks ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altcoins = ['ETH','LTC','XRP','ETC','STR','DASH','SC','XMR','XEM']\n",
    "\n",
    "altcoin_data = {}\n",
    "for altcoin in altcoins:\n",
    "    coinpair = 'BTC_{}'.format(altcoin)\n",
    "    crypto_price_df = get_crypto_data(coinpair)\n",
    "    altcoin_data[altcoin] = crypto_price_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altcoin_data['XEM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.3 - Convert Prices to USD\n",
    "\n",
    "Now we can combine this BTC-altcoin exchange rate data with our Bitcoin pricing index to directly calculate the historical USD values for each altcoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate USD Price as a new column in each altcoin dataframe\n",
    "for altcoin in altcoin_data.keys():\n",
    "    altcoin_data[altcoin]['price_usd'] =  altcoin_data[altcoin]['weightedAverage'] * btc_usd_datasets['avg_btc_price_usd']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we've created a new column in each altcoin dataframe with the USD prices for that coin.\n",
    "\n",
    "Next, we can re-use our merge_dfs_on_column function from earlier to create a combined dataframe of the USD price for each cryptocurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge USD price of each altcoin into single dataframe \n",
    "combined_df = merge_dfs_on_column(list(altcoin_data.values()), list(altcoin_data.keys()), 'price_usd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy. Now let's also add the Bitcoin prices as a final column to the combined dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add BTC price to the dataframe\n",
    "combined_df['BTC'] = btc_usd_datasets['avg_btc_price_usd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should have a single dataframe containing daily USD prices for the ten cryptocurrencies that we're examining.\n",
    "\n",
    "Let's reuse our df_scatter function from earlier to chart all of the cryptocurrency prices against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart all of the altocoin prices\n",
    "df_scatter(combined_df, 'Cryptocurrency Prices (USD)', seperate_y_axis=False, y_axis_label='Coin Value (USD)', scale='log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.4 - Perform Correlation Analysis\n",
    "\n",
    "You might notice is that the cryptocurrency exchange rates, despite their wildly different values and volatility, look slightly correlated. Especially since the spike in April 2017, even many of the smaller fluctuations appear to be occurring in sync across the entire market.\n",
    "\n",
    "A visually-derived hunch is not much better than a guess until we have the stats to back it up.\n",
    "\n",
    "We can test our correlation hypothesis using the Pandas corr() method, which computes a Pearson correlation coefficient for each column in the dataframe against each other column.\n",
    "\n",
    "Computing correlations directly on a non-stationary time series (such as raw pricing data) can give biased correlation values. We will work around this by first applying the pct_change() method, which will convert each cell in the dataframe from an absolute price value to a daily return percentage.\n",
    "\n",
    "First we'll calculate correlations for 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the pearson correlation coefficients for cryptocurrencies in 2016\n",
    "combined_df_2016 = combined_df[combined_df.index.month == 10]\n",
    "combined_df_2016.pct_change().corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These correlation coefficients are all over the place. Coefficients close to 1 or -1 mean that the series' are strongly correlated or inversely correlated respectively, and coefficients close to zero mean that the values are not correlated, and fluctuate independently of each other.\n",
    "\n",
    "To help visualize these results, we'll create one more helper visualization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heatmap(df, title, absolute_bounds=True):\n",
    "    '''Plot a correlation heatmap for the entire dataframe'''\n",
    "    heatmap = go.Heatmap(\n",
    "        z=df.corr(method='pearson').as_matrix(),\n",
    "        x=df.columns,\n",
    "        y=df.columns,\n",
    "        colorbar=dict(title='Pearson Coefficient'),\n",
    "    )\n",
    "    \n",
    "    layout = go.Layout(title=title)\n",
    "    \n",
    "    if absolute_bounds:\n",
    "        heatmap['zmax'] = 1.0\n",
    "        heatmap['zmin'] = -1.0\n",
    "        \n",
    "    fig = go.Figure(data=[heatmap], layout=layout)\n",
    "    py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_heatmap(combined_df_2016.pct_change(), \"Cryptocurrency Correlations in 2016\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the dark red values represent strong correlations (note that each currency is, obviously, strongly correlated with itself), and the dark blue values represent strong inverse correlations. All of the light blue/orange/gray/tan colors in-between represent varying degrees of weak/non-existent correlations.\n",
    "\n",
    "What does this chart tell us? Essentially, it shows that there was little statistically significant linkage between how the prices of different cryptocurrencies fluctuated during 2016.\n",
    "\n",
    "Now, to test our hypothesis that the cryptocurrencies have become more correlated in recent months, let's repeat the same test using only the data from 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_2017 = combined_df[combined_df.index.month == 11]\n",
    "combined_df_2017.pct_change().corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_heatmap(combined_df_2017.pct_change(), \"Cryptocurrency Correlations in 2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
